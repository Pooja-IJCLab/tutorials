{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced usage of HiPS and MOCs to explore complex regions of interest\n",
    "\n",
    "Stefania Amodea¹, Matthieu Baumann¹, Thomas Boch¹, Caroline Bot¹, Katarina A. Lutz¹, Manon Marchand¹. \n",
    "\n",
    "1. Université de Strasbourg, CNRS, Observatoire Astronomique de Strasbourg, UMR 7550, F-67000, Strasbourg, France\n",
    "\n",
    "Thomas Boch and Caroline Bot wrote the original version of this tutorial, available on the [EURO-VO tutorials page](http://www.euro-vo.org/?q=science/scientific-tutorials). It was presented at the workshop \"[Detecting the unexpected, Discovery in the Era of Astronomically Big Data](https://www.lsstcorporation.org/node/107)\". The version here is an adaptation to jupyter notebooks by the Strasbourg astronomical Data Center ([CDS](https://cdsweb.u-strasbg.fr/)) team.\n",
    "\n",
    "*** \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates an advanced usage of Hierarchical Progressive Surveys ([HiPS](https://ui.adsabs.harvard.edu/abs/2017ivoa.spec.0519F/abstract)) and Multi-Order Coverage ([MOC](https://www.ivoa.net/documents/MOC/)) maps. Wis this tutorial, you will: \n",
    "\n",
    "1. learn how to find fits files\n",
    "2. build a map of the coverage of all the fits files \n",
    "3. select only the parts of this map that correspond to low-extinction regions\n",
    "4. retrieve objects from large catalogs inside the obtained map -- that has a non-trivial shape and non-necessarily-connected regions\n",
    "5. combine catalogs to visualize a color-color diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General python packages\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# For vizualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Packages specific to astronomy tasks\n",
    "from astropy.io import fits\n",
    "from astropy.coordinates import SkyCoord, Angle, match_coordinates_sky\n",
    "import astropy.units as u\n",
    "import mocpy\n",
    "import pyvo\n",
    "import healpy as hp\n",
    "from astroquery.vizier import Vizier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Finding the images\n",
    "\n",
    "We want to find all Short-Red images in the Macquarie/AAO/Strasbourg Hα Planetary Galactic catalog ([MASH](https://vizier.cfa.harvard.edu/vizier/MASH/index.htx)) using the VizieR associated data service.\n",
    "\n",
    "<img src=Data/images/logos/vizier.svg alt=\"vizier's logo\" style=\"height:100px; display: inline-block;\"/>\n",
    "\n",
    "The [VizieR](https://vizier.cds.unistra.fr/) service at CDS inventories astronomical catalogs published in the literature. Some of these catalogs contain data associated with publications and the tables therein. This data can be browsed and explored through the VizieR-associated data service, linked to the traditional VizieR table service. Here we look for images associated with the MASH catalog of planetary nebulae ([Parker *et. al.* 2006-2008](https://ui.adsabs.harvard.edu/abs/2006yCat.5127....0P/abstract)). The MASH fits files are cut-outs extracted from a larger Hα and Short Red survey to constitute a set of regions of interest around planetary nebulae. \n",
    "\n",
    "To find VizieR-associated data, we use the Table Access Protocol (TAP) with the VizieR endpoint. Through the VizieR TAP endpoint, we can search for tables, their content, and information on associated data. \n",
    "\n",
    "First, we search for the MASH catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the address of the service, you can also directly visit the website\n",
    "tap_vizier = pyvo.dal.TAPService(\n",
    "    'https://tapvizier.cds.unistra.fr/TAPVizieR/tap')\n",
    "\n",
    "# a query that searches for all tables with the words MASH and Parker in their description\n",
    "query = '''\n",
    "        SELECT  *  FROM tables \n",
    "        WHERE description LIKE '%MASH%Parker%'\n",
    "        '''\n",
    "\n",
    "mash_catalogues = tap_vizier.search(query).to_table()\n",
    "mash_catalogues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are interested in the tables belonging to the catalogs `V/127A`. This includes tables `V/127A/mash1` and `V/127A/mash2`. To have a look at the content of these tables, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "        SELECT TOP 5 * FROM \\\"V/127A/mash1\\\" \n",
    "        '''\n",
    "mash1_head = tap_vizier.search(query).to_table()\n",
    "mash1_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the last column of this table is called `AssocData` and contains the entry `fits`. If you look at this table on the VizieR web interface, you can download the associated fits file. Within this notebook, we query the `obscore` database to get the URLs to the fits file. Using the `astropy.io.fits` module, we can open the fits files from their URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_tap_vizier = pyvo.dal.TAPService('https://cdsarc.cds.unistra.fr/saadavizier.tap/tap')\n",
    "query = \"\"\"\n",
    "        SELECT TOP 5  *  FROM obscore \n",
    "        WHERE obs_collection='V/127A'   \n",
    "        \"\"\"\n",
    "mash_fits = obs_tap_vizier.search(query).to_table()\n",
    "mash_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result from this query provides information on the fits files associated with the MASH catalogs. In particular, the column `access_url` provides the location of the data. To get the first image we could do:\n",
    "\n",
    "`image = fits.open(mash_fits['access_url'][0])`\n",
    "\n",
    "and then work on the image, plot it or save it to our machine. However, downloading all the data takes quite some time. For this tutorial, **we prepared a subsample of 335 of these Short Red images that will run promptly** but we encourage you to try accessing the full Short Red sample on your own later. The subsample is available either at http://astro.u-strasbg.fr/~bot/BochBot.tar.gz or in the Data Folder of this repository. If you run this tutorial on Binder, you do not need to download anything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a MOC of the MASH images\n",
    "\n",
    "The multi-order coverage (MOC) map of a set of images represents their sky coverage. MOCs can describe arbitrary zones in the sky which do not need to be connected. You'll see that the union or intersection of two MOCs requires few time and computational effort. Catalogs can also be filtered by MOCs. \n",
    "\n",
    "Here we want to use the fits files just downloaded to create a MOC map corresponding to the coverage of the MASH images.\n",
    "\n",
    "### Organising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to find fits images downloaded from the archive above\n",
    "datadir = Path('Data/MASH_Sample/') \n",
    "datadir.mkdir(parents=True, exist_ok=True)\n",
    "# Make new directory where to save MOCs\n",
    "outdir = Path('Data/Result_moc/')        \n",
    "outdir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, we could ignore the next cell.  However, some possible deprecated keywords in the fits header would hamper the MOC creation and would cause errors in the underlying `astropy.wcs.WCS` module. This is why we rewrite the headers of the fits files so that they only contain the useful keywords needed to define the coordinate frame correctly before using `mocpy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all fits files in datadir\n",
    "mash_file_list = datadir.glob('*_sr.fits')\n",
    "\n",
    "# Modify the header so that the MOC creation works smoothly\n",
    "for file_name in mash_file_list:\n",
    "    ima = fits.open(file_name) \n",
    "    hdr = ima[0].header\n",
    "    needed_keywords = ['SIMPLE','BITPIX', 'NAXIS','EXTEND',                     # basic fits keywords \n",
    "                       'NAXIS1', 'NAXIS2', 'RADECSYS',                          # essential WCS keywords\n",
    "                       'CD1_1','CD1_2','CD2_1','CD2_2',                         \n",
    "                       'CTYPE1', 'CUNIT1', 'CDELT1', 'CRPIX1', 'CRVAL1', \n",
    "                       'CTYPE2', 'CUNIT2', 'CDELT2', 'CRPIX2', 'CRVAL2',\n",
    "                       'EQUINOX', 'BLANK'                                       # used additionally by mocpy\n",
    "                        ]\n",
    "    new_hdr = fits.Header()\n",
    "    for keyword in needed_keywords:\n",
    "        try:\n",
    "            new_hdr[keyword] = ima[0].header[keyword]\n",
    "        except KeyError:\n",
    "            continue\n",
    "    new_hdr['RADESYS'] = new_hdr['RADECSYS']\n",
    "    fits.writeto(outdir / Path(file_name.stem + '_modified.fits'), \n",
    "                 ima[0].data, new_hdr, output_verify='fix', overwrite=True)\n",
    "    ima.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the MOC\n",
    "\n",
    "Here we can create the MOC of the MASH images with the `MOCPy` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the updated files and create the MOC    \n",
    "mash_file_list = outdir.glob('*_sr_modified.fits')\n",
    "moc_mash = mocpy.MOC.from_fits_images(mash_file_list, max_norder=15)\n",
    "moc_mash.write(outdir / 'mash_moc.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the MOC\n",
    "\n",
    "We'll use the MOCpy `World2ScreenMPL` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_centre = SkyCoord('21:49:00', '-02:30:00', \n",
    "                      unit=(u.hourangle, u.deg), frame='galactic')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "with mocpy.World2ScreenMPL(fig, fov=2.5 * u.deg,\n",
    "                           center=fig_centre,\n",
    "                           coordsys=\"galactic\", \n",
    "                           rotation=Angle(0, u.degree),\n",
    "                           projection=\"AIT\") as wcs:\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=wcs)\n",
    "    moc_mash.fill(ax=ax, wcs=wcs, alpha=0.5, fill=True, color='purple')\n",
    "    ax.set_xlabel('RA')\n",
    "    ax.set_ylabel('Dec')\n",
    "    lon, lat = ax.coords[0], ax.coords[1]\n",
    "    lon.set_major_formatter('hh:mm:ss')\n",
    "    lat.set_major_formatter('dd:mm')\n",
    "    lon.set_ticklabel(exclude_overlapping=True)\n",
    "    lat.set_ticklabel(exclude_overlapping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure only shows a small region on the Sky, but you can see how the MOC has arbitrary shapes and not all regions are connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load an archival extinction map and create the MOC of the low extinction regions\n",
    "\n",
    "Different works (e.g. [Schlegel et al. 1998](https://iopscience.iop.org/article/10.1086/305772), [Schlafly  & Finkbeiner 2011](https://ui.adsabs.harvard.edu/abs/2011AAS...21831803S/abstract), [Green et al. 2015](https://iopscience.iop.org/article/10.1088/0004-637X/810/1/25)...) have created extinction maps of the sky that are publicly available. Some of these maps are all-sky maps, while others have higher resolutions, or come from different methods... They can be found in HEALPix format (among others) on the Legacy Archive for Microwave Background Data Analysis ([LAMBDA](https://lambda.gsfc.nasa.gov/)) website or on the Analysis Center for Extended Data ([CADE](http://cade.irap.omp.eu/dokuwiki/doku.php?id=start)) website. \n",
    "\n",
    "For this tutorial, we will download the well-known all-sky extinction map from Schlegel *et al.* from the LAMBDA  website and define the low extinction area for which $0 < E(B-V) < 0.5$ as a MOC. It has an [information page](https://lambda.gsfc.nasa.gov/product/foreground/fg_sfd_info.html).\n",
    "\n",
    "The map is available here: https://lambda.gsfc.nasa.gov/data/foregrounds/SFD/lambda_sfd_ebv.fits and we save it to disc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_map = fits.open('https://lambda.gsfc.nasa.gov/data/foregrounds/SFD/' + \n",
    "                    'lambda_sfd_ebv.fits')\n",
    "ext_map.writeto(outdir / 'Schlegel_extinction_map.fits', overwrite=True)\n",
    "ext_map.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in regions with low extinction. So we aim to get a MOC of all regions where the extinction values from the Schlegel *et al.* map are between 0 and 0.5mag. The extinction map we got from the NASA webpage is in the HEALPix format. This is an efficient presentation of all-sky maps. The HEALPix tesselation is also used by MOCs. So to get the MOC from the extinction map, we do the following. \n",
    "\n",
    "First, we check the coordinate system in the map header. We will need to convert to equatorial coordinates, change the projection of the map, and set the order (*i.e.* resolution) of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the downloaded FITS file\n",
    "hdul = fits.open(outdir / 'Schlegel_extinction_map.fits')  \n",
    "hdr = hdul[0].header\n",
    "hdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nside = hdul[0].header['NSIDE']\n",
    "norder = hdul[0].header['RESOLUTN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header allows to see that this map is in galactic coordinates. We will need to convert this into equatorial coordinates to compare with our other maps.\n",
    "\n",
    "To do so, we'll use the function `rotate_map_pixel` of healpy that only accepts a ring-ordered map. This is why we load the map in a ring order, apply the coordinate change, and then transfer back to the nested ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the map in the ringed Healpix ordering\n",
    "extinction_map = hp.read_map('Data/Schlegel_extinction_map.fits', nest=False)\n",
    "# Define a rotation from galactic to equatorial coordinates\n",
    "r = hp.Rotator(coord=['G','C'])\n",
    "# Do the coordinate rotation\n",
    "extinction_map_equatorial_ring = r.rotate_map_pixel(extinction_map)\n",
    "# Switch back to nested ordering\n",
    "extinction_map_equatorial = hp.reorder(extinction_map_equatorial_ring, r2n=True) #reorder from RING to NESTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(extinction_map_equatorial, bins=100);\n",
    "print(f\"\"\"Min extinction: {extinction_map_equatorial.min()},\n",
    "Max extinction: {extinction_map_equatorial.max()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we declare which pixel we want to use, let's take all pixels with an extinction lower than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_pixels_to_keep = np.where(extinction_map_equatorial<0.5)[0]\n",
    "\n",
    "# without the \"[0]\", np.where returns a tuple where the first element is the array of indices\n",
    "# we need the array to pass to the MOC creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we brought the extinction map in the right shape, the indexes of the pixels we want to keep are the indexes of the interesting Healpix cells. Thus the MOC is created by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moc_low_extinction = mocpy.MOC.from_healpix_cells(indexes_pixels_to_keep,\n",
    "                                           np.full((len(indexes_pixels_to_keep,)), norder))\n",
    "moc_low_extinction.save(str(outdir / 'low_extinction_moc.fits'), format=\"fits\", overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Find out which regions are covered by the MASH short-red images in the low extinction regions defined above\n",
    "\n",
    "To find out the sky regions of the MASH sample that are at low extinction, we build the intersection of the two MOCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moc_intersection = moc_low_extinction.intersection(moc_mash)\n",
    "# Once the intersection is bluit, we can for example print the sky fraction :\n",
    "print(f'The intersection of the two MOCs covers {round(moc_intersection.sky_fraction * 100, 4)}% of the sky')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the coverage of the two MOCs and their intersection. The grey area is where the extinction is low. The blue one is the MASH coverage. The tiny red dots show the MASH coverage in low extinction regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(111, figsize=(10, 8))\n",
    "with mocpy.World2ScreenMPL(fig, fov=200 * u.deg,\n",
    "                           center=SkyCoord(200, -20, unit='deg', frame='icrs'),\n",
    "                           coordsys=\"icrs\",\n",
    "                           rotation=Angle(0, u.degree),\n",
    "                           projection=\"AIT\") as wcs:\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=wcs)\n",
    "    moc_low_extinction.fill(ax=ax, wcs=wcs, alpha=0.5, fill=True, color=\"grey\",label='low extinction')\n",
    "    moc_mash.fill(ax=ax, wcs=wcs, alpha=0.5, fill=True, color=\"dodgerblue\", label='MASH coverage')\n",
    "    moc_intersection.fill(ax=ax, wcs=wcs, alpha=0.5, fill=True, color=\"crimson\", label='MASH in low extinction reg.')\n",
    "    # Sets labels\n",
    "    ax.set_xlabel('RA')\n",
    "    ax.set_ylabel('Dec')\n",
    "    # Sets ticks\n",
    "    lon, lat = ax.coords[0], ax.coords[1]\n",
    "    lon.set_major_formatter('hh:mm:ss')\n",
    "    lat.set_major_formatter('dd:mm')\n",
    "    lon.set_ticklabel(exclude_overlapping=True)\n",
    "    lat.set_ticklabel(exclude_overlapping=True)\n",
    "    lon.set_ticks(spacing=2 * u.hourangle)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query the 2MASS and Gaia Catalogues by MOC\n",
    "\n",
    "Without the usage of MOC, querying for sources in the low extinction regions covered by the MASH subsample would be tedious or even impossible. Indeed, one would need to load the whole catalog and make selections which would not be possible given the size of some catalogs. Alternatively, one would need to query the catalog field by field, which would take time and several queries. Instead, here we will use the power of MOC files to query large catalogs directly in the covered regions only. We will use coverages of the low extinction and MASH regions to query for sources from the Gaia and 2MASS surveys in these highly non-continuous and non-trivial shape areas.\n",
    "\n",
    "First, let's see which Gaia and 2MASS catalogs are available on VizieR. We could, as above, use the TAP endpoint of VizieR. But we show below the `Vizier` module in the `astroquery` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "catalog_list_twomass = Vizier.find_catalogs('Cutri')\n",
    "for k, v in catalog_list_twomass.items():\n",
    "    print(k, ': ', v.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_list_gaia = Vizier.find_catalogs('Gaia DR2', max_catalogs=1000)\n",
    "for k, v in catalog_list_gaia.items():\n",
    "    print(k, ': ', v.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2MASS we will want to use `II/246 :  2MASS All-Sky Catalog of Point Sources (Cutri+ 2003)` and for Gaia `I/345 :  Gaia DR2 (Gaia Collaboration, 2018)`. Before we query the full two tables we only look at a few sources for each table to understand which columns are available. The query below will give us 50 sources each -- the default for the `get_catalogs` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_twomass = Vizier.get_catalogs('II/246')\n",
    "print(test_twomass)\n",
    "test_twomass[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gaia = Vizier.get_catalogs('I/345')\n",
    "print(test_gaia)\n",
    "test_gaia[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see below, we only need coordinates, 2MASS photometry in the H and K band, and Gaia photometry in the Gaia G band. So we'll query the tables `II/246/out` for 2MASS and `I/345/gaia2` for Gaia DR2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twomass = moc_intersection.query_vizier_table('II/246/out', max_rows=20000)\n",
    "twomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia = moc_intersection.query_vizier_table('I/345/gaia2', max_rows=20000)\n",
    "gaia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cross-match Gaia and WISE sources in all fields\n",
    "\n",
    "We now want to find sources in the selected region (observed in the MASH regions of interest and at low extinction) that are common to the Wide Infrared Survey Explorer ([WISE](https://irsa.ipac.caltech.edu/Missions/wise.html)) and Gaia catalogs. To do so, we will perform a cross-match of the Gaia and WISE catalogs. Alternatively, we could use the CDS XMatch service via the corresponding `astroquery` module.\n",
    "\n",
    "To do so, let's first inspect the `match_coordinates_sky` function from `astropy.coordinates`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(match_coordinates_sky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate the coordinates in the appropriate format\n",
    "twomass_coord = SkyCoord(ra=twomass['RAJ2000'], dec=twomass['DEJ2000'], unit=u.deg)\n",
    "gaia_coord = SkyCoord(ra=gaia['ra_epoch2000'], \n",
    "                      dec=gaia['dec_epoch2000'], unit=u.deg)\n",
    "\n",
    "index, separation_2d, _ = match_coordinates_sky(twomass_coord, gaia_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide the maximum separation between objects to be considered acceptable matches\n",
    "max_separation = 1.0 * u.arcsec\n",
    "# Apply constraint on the two catalogs\n",
    "sep_constraint = separation_2d < max_separation\n",
    "twomass_matches = twomass[sep_constraint]\n",
    "gaia_matches = gaia[index[sep_constraint]]\n",
    "# Select only interesting columns from twomass_matches\n",
    "match_catalog = twomass_matches['_2MASS', 'RAJ2000', 'DEJ2000', 'Hmag', 'Kmag']\n",
    "# Add column G magnitude from gaia\n",
    "match_catalog['Gmag'] = gaia_matches['phot_g_mean_mag']\n",
    "match_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build a color-color diagram\n",
    "\n",
    "We now use the data we got from the cross-match to get a WISE/Gaia color-color diagram for all the sources in the low extinction sky regions covered by the MASH survey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.plot(match_catalog['Hmag'] - match_catalog['Kmag'], \n",
    "        match_catalog['Gmag'] - match_catalog['Hmag'], linestyle='', marker='.')\n",
    "ax.set_xlabel('H - Ks [mag]',fontsize=16)\n",
    "ax.set_ylabel('G - H [mag]',fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
